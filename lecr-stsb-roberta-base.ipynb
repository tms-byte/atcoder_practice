{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-08T05:19:37.186866Z","iopub.execute_input":"2023-02-08T05:19:37.187826Z","iopub.status.idle":"2023-02-08T05:19:37.322830Z","shell.execute_reply.started":"2023-02-08T05:19:37.187721Z","shell.execute_reply":"2023-02-08T05:19:37.321742Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/lecr-ensemble-data1/xlm-roberta-base_fold0_42.pth\n/kaggle/input/lecr-ensemble-data1/sentence-transformers-all-mpnet-base-v2/model/config.json\n/kaggle/input/lecr-ensemble-data1/sentence-transformers-all-mpnet-base-v2/model/pytorch_model.bin\n/kaggle/input/lecr-ensemble-data1/sentence-transformers-all-mpnet-base-v2/tokenizer/tokenizer.json\n/kaggle/input/lecr-ensemble-data1/sentence-transformers-all-mpnet-base-v2/tokenizer/tokenizer_config.json\n/kaggle/input/lecr-ensemble-data1/sentence-transformers-all-mpnet-base-v2/tokenizer/special_tokens_map.json\n/kaggle/input/lecr-ensemble-data1/sentence-transformers-all-mpnet-base-v2/tokenizer/vocab.txt\n/kaggle/input/lecr-ensemble-data1/sentence-transformers-all-mpnet-base-v2/config/config.json\n/kaggle/input/lecr-ensemble-data1/sentence-transformers-all-MiniLM-L6-v2/model/config.json\n/kaggle/input/lecr-ensemble-data1/sentence-transformers-all-MiniLM-L6-v2/model/pytorch_model.bin\n/kaggle/input/lecr-ensemble-data1/sentence-transformers-all-MiniLM-L6-v2/tokenizer/tokenizer.json\n/kaggle/input/lecr-ensemble-data1/sentence-transformers-all-MiniLM-L6-v2/tokenizer/tokenizer_config.json\n/kaggle/input/lecr-ensemble-data1/sentence-transformers-all-MiniLM-L6-v2/tokenizer/special_tokens_map.json\n/kaggle/input/lecr-ensemble-data1/sentence-transformers-all-MiniLM-L6-v2/tokenizer/vocab.txt\n/kaggle/input/lecr-ensemble-data1/sentence-transformers-all-MiniLM-L6-v2/config/config.json\n/kaggle/input/lecr-ensemble-data1/xlm-roberta-base/model/config.json\n/kaggle/input/lecr-ensemble-data1/xlm-roberta-base/model/pytorch_model.bin\n/kaggle/input/lecr-ensemble-data1/xlm-roberta-base/tokenizer/tokenizer.json\n/kaggle/input/lecr-ensemble-data1/xlm-roberta-base/tokenizer/tokenizer_config.json\n/kaggle/input/lecr-ensemble-data1/xlm-roberta-base/tokenizer/special_tokens_map.json\n/kaggle/input/lecr-ensemble-data1/xlm-roberta-base/tokenizer/sentencepiece.bpe.model\n/kaggle/input/lecr-ensemble-data1/xlm-roberta-base/config/config.json\n/kaggle/input/paraphrasemultilingualmpnetbasev4-2/paraphrase-multilingual-mpnet-base-v4/README.md\n/kaggle/input/paraphrasemultilingualmpnetbasev4-2/paraphrase-multilingual-mpnet-base-v4/model/sentence_bert_config.json\n/kaggle/input/paraphrasemultilingualmpnetbasev4-2/paraphrase-multilingual-mpnet-base-v4/model/pytorch_model.bin\n/kaggle/input/paraphrasemultilingualmpnetbasev4-2/paraphrase-multilingual-mpnet-base-v4/model/config_sentence_transformers.json\n/kaggle/input/paraphrasemultilingualmpnetbasev4-2/paraphrase-multilingual-mpnet-base-v4/model/modules.json\n/kaggle/input/paraphrasemultilingualmpnetbasev4-2/paraphrase-multilingual-mpnet-base-v4/model/sentencepiece.bpe.model\n/kaggle/input/paraphrasemultilingualmpnetbasev4-2/paraphrase-multilingual-mpnet-base-v4/tokenizer/tokenizer.json\n/kaggle/input/paraphrasemultilingualmpnetbasev4-2/paraphrase-multilingual-mpnet-base-v4/tokenizer/tokenizer_config.json\n/kaggle/input/paraphrasemultilingualmpnetbasev4-2/paraphrase-multilingual-mpnet-base-v4/tokenizer/special_tokens_map.json\n/kaggle/input/paraphrasemultilingualmpnetbasev4-2/paraphrase-multilingual-mpnet-base-v4/config/config.json\n/kaggle/input/paraphrasemultilingualmpnetbasev4-2/paraphrase-multilingual-mpnet-base-v4/config/sentence_bert_config.json\n/kaggle/input/paraphrasemultilingualmpnetbasev4-2/paraphrase-multilingual-mpnet-base-v4/config/config_sentence_transformers.json\n/kaggle/input/paraphrasemultilingualmpnetbasev4-2/paraphrase-multilingual-mpnet-base-v4/config/modules.json\n/kaggle/input/paraphrasemultilingualmpnetbasev4-2/paraphrase-multilingual-mpnet-base-v4/config/sentencepiece.bpe.model\n/kaggle/input/paraphrasemultilingualmpnetbasev4-2/paraphrase-multilingual-mpnet-base-v4/1_Pooling/config.json\n/kaggle/input/paraphrasedistilrobertabasev1exp2/paraphrase-distilroberta-base-v1-exp2/README.md\n/kaggle/input/paraphrasedistilrobertabasev1exp2/paraphrase-distilroberta-base-v1-exp2/model/config.json\n/kaggle/input/paraphrasedistilrobertabasev1exp2/paraphrase-distilroberta-base-v1-exp2/model/merges.txt\n/kaggle/input/paraphrasedistilrobertabasev1exp2/paraphrase-distilroberta-base-v1-exp2/model/sentence_bert_config.json\n/kaggle/input/paraphrasedistilrobertabasev1exp2/paraphrase-distilroberta-base-v1-exp2/model/pytorch_model.bin\n/kaggle/input/paraphrasedistilrobertabasev1exp2/paraphrase-distilroberta-base-v1-exp2/model/config_sentence_transformers.json\n/kaggle/input/paraphrasedistilrobertabasev1exp2/paraphrase-distilroberta-base-v1-exp2/model/modules.json\n/kaggle/input/paraphrasedistilrobertabasev1exp2/paraphrase-distilroberta-base-v1-exp2/tokenizer/tokenizer.json\n/kaggle/input/paraphrasedistilrobertabasev1exp2/paraphrase-distilroberta-base-v1-exp2/tokenizer/vocab.json\n/kaggle/input/paraphrasedistilrobertabasev1exp2/paraphrase-distilroberta-base-v1-exp2/tokenizer/tokenizer_config.json\n/kaggle/input/paraphrasedistilrobertabasev1exp2/paraphrase-distilroberta-base-v1-exp2/tokenizer/special_tokens_map.json\n/kaggle/input/paraphrasedistilrobertabasev1exp2/paraphrase-distilroberta-base-v1-exp2/config/config.json\n/kaggle/input/paraphrasedistilrobertabasev1exp2/paraphrase-distilroberta-base-v1-exp2/config/merges.txt\n/kaggle/input/paraphrasedistilrobertabasev1exp2/paraphrase-distilroberta-base-v1-exp2/config/sentence_bert_config.json\n/kaggle/input/paraphrasedistilrobertabasev1exp2/paraphrase-distilroberta-base-v1-exp2/config/config_sentence_transformers.json\n/kaggle/input/paraphrasedistilrobertabasev1exp2/paraphrase-distilroberta-base-v1-exp2/config/modules.json\n/kaggle/input/paraphrasedistilrobertabasev1exp2/paraphrase-distilroberta-base-v1-exp2/1_Pooling/config.json\n/kaggle/input/learning-equality-curriculum-recommendations/sample_submission.csv\n/kaggle/input/learning-equality-curriculum-recommendations/topics.csv\n/kaggle/input/learning-equality-curriculum-recommendations/correlations.csv\n/kaggle/input/learning-equality-curriculum-recommendations/content.csv\n/kaggle/input/stsbrobertabasev2exp2/stsb-roberta-base-v2-exp1/README.md\n/kaggle/input/stsbrobertabasev2exp2/stsb-roberta-base-v2-exp1/model/config.json\n/kaggle/input/stsbrobertabasev2exp2/stsb-roberta-base-v2-exp1/model/merges.txt\n/kaggle/input/stsbrobertabasev2exp2/stsb-roberta-base-v2-exp1/model/sentence_bert_config.json\n/kaggle/input/stsbrobertabasev2exp2/stsb-roberta-base-v2-exp1/model/pytorch_model.bin\n/kaggle/input/stsbrobertabasev2exp2/stsb-roberta-base-v2-exp1/model/config_sentence_transformers.json\n/kaggle/input/stsbrobertabasev2exp2/stsb-roberta-base-v2-exp1/model/modules.json\n/kaggle/input/stsbrobertabasev2exp2/stsb-roberta-base-v2-exp1/tokenizer/tokenizer.json\n/kaggle/input/stsbrobertabasev2exp2/stsb-roberta-base-v2-exp1/tokenizer/vocab.json\n/kaggle/input/stsbrobertabasev2exp2/stsb-roberta-base-v2-exp1/tokenizer/tokenizer_config.json\n/kaggle/input/stsbrobertabasev2exp2/stsb-roberta-base-v2-exp1/tokenizer/special_tokens_map.json\n/kaggle/input/stsbrobertabasev2exp2/stsb-roberta-base-v2-exp1/config/config.json\n/kaggle/input/stsbrobertabasev2exp2/stsb-roberta-base-v2-exp1/config/merges.txt\n/kaggle/input/stsbrobertabasev2exp2/stsb-roberta-base-v2-exp1/config/sentence_bert_config.json\n/kaggle/input/stsbrobertabasev2exp2/stsb-roberta-base-v2-exp1/config/config_sentence_transformers.json\n/kaggle/input/stsbrobertabasev2exp2/stsb-roberta-base-v2-exp1/config/modules.json\n/kaggle/input/stsbrobertabasev2exp2/stsb-roberta-base-v2-exp1/1_Pooling/config.json\n","output_type":"stream"}]},{"cell_type":"markdown","source":"I added stsb-roberta-base　model to https://www.kaggle.com/code/bulivington/retriever-ensemble-and-postprocessing . So I added class CFG3 and changed CFG_list.\n\nReference\n\nhttps://www.kaggle.com/code/bulivington/retriever-ensemble-and-postprocessing\n\nhttps://www.kaggle.com/code/utm529fg/tuning-paraphrase-multilingual-mpnet-base-0-331\n\n","metadata":{}},{"cell_type":"markdown","source":"* 2ステージのプロセスを得る。\n1. 1ステージは教師なしモデルによる候補の選択、ここでは訓練は行わない。否、行っていた。exp1は。\n2. そして2ステージ目がreranker、つまり順位付で重要なトピックの分類。これは別のノートで作成した訓練済みモデルを使用している。\n* xlm-roberta-base は共通のモデルであり、1種類のtransformerによる埋め込みベクトルから学習したモデルを使用している。各のtransformerで学習させた重みをロードすると？\n* xlm-roberta-largeをアンサンブルした2種類の推論を行うとどうなるか？","metadata":{}},{"cell_type":"code","source":"# =================================================================================\n# Libraries\n# =========================================================================================\nimport os\nimport gc\nimport time\nimport math\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.checkpoint import checkpoint\nimport tokenizers\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_cosine_schedule_with_warmup, DataCollatorWithPadding\nimport cupy as cp\nfrom cuml.metrics import pairwise_distances\nfrom cuml.neighbors import NearestNeighbors\n%env TOKENIZERS_PARALLELISM=false\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass CFG1:\n    uns_model = \"/kaggle/input/lecr-ensemble-data1/sentence-transformers-all-mpnet-base-v2\"\n    sup_model = \"/kaggle/input/lecr-ensemble-data1/xlm-roberta-base\"\n    uns_tokenizer = AutoTokenizer.from_pretrained(uns_model + '/tokenizer')\n    sup_tokenizer = AutoTokenizer.from_pretrained(sup_model + '/tokenizer')\n    pooling = \"mean\"\n    gradient_checkpointing = False\n    add_with_best_prob = True\n\n# このunsモデルは最高性能である。\nclass CFG2:\n    uns_model = \"/kaggle/input/lecr-ensemble-data1/sentence-transformers-all-MiniLM-L6-v2\"\n    sup_model = \"/kaggle/input/lecr-ensemble-data1/xlm-roberta-base\"\n    uns_tokenizer = AutoTokenizer.from_pretrained(uns_model + '/tokenizer')\n    sup_tokenizer = AutoTokenizer.from_pretrained(sup_model + '/tokenizer')\n    pooling = \"mean\"\n    gradient_checkpointing = False\n    add_with_best_prob = True\n    \nclass CFG3:\n    uns_model = \"/kaggle/input/stsbrobertabasev2exp2/stsb-roberta-base-v2-exp1\"\n    sup_model = \"/kaggle/input/lecr-ensemble-data1/xlm-roberta-base\"\n    uns_tokenizer = AutoTokenizer.from_pretrained(uns_model + '/tokenizer')\n    sup_tokenizer = AutoTokenizer.from_pretrained(sup_model + '/tokenizer')\n    pooling = \"mean\"\n    gradient_checkpointing = False\n    add_with_best_prob = True\n\n# LARGE\nclass CFG4:\n    uns_model = \"/kaggle/input/lecr-ensemble-data1/sentence-transformers-all-mpnet-base-v2\"\n    sup_model = \"/kaggle/input/lecr-ensemble-data1/xlm-roberta-large\"\n    uns_tokenizer = AutoTokenizer.from_pretrained(uns_model + '/tokenizer')\n    sup_tokenizer = AutoTokenizer.from_pretrained(sup_model + '/tokenizer')\n    pooling = \"mean\"\n    gradient_checkpointing = False\n    add_with_best_prob = True\n\n# このunsモデルは最高性能である。\nclass CFG5:\n    uns_model = \"/kaggle/input/lecr-ensemble-data1/sentence-transformers-all-MiniLM-L6-v2\"\n    sup_model = \"/kaggle/input/lecr-ensemble-data1/xlm-roberta-large\"\n    uns_tokenizer = AutoTokenizer.from_pretrained(uns_model + '/tokenizer')\n    sup_tokenizer = AutoTokenizer.from_pretrained(sup_model + '/tokenizer')\n    pooling = \"mean\"\n    gradient_checkpointing = False\n    add_with_best_prob = True\n    \nclass CFG6:\n    uns_model = \"/kaggle/input/stsbrobertabasev2exp2/stsb-roberta-base-v2-exp1\"\n    sup_model = \"/kaggle/input/lecr-ensemble-data1/xlm-roberta-large\"\n    uns_tokenizer = AutoTokenizer.from_pretrained(uns_model + '/tokenizer')\n    sup_tokenizer = AutoTokenizer.from_pretrained(sup_model + '/tokenizer')\n    pooling = \"mean\"\n    gradient_checkpointing = False\n    add_with_best_prob = True\n\n\n    \n\"\"\"\nclass CFG4:\n    uns_model = \"/kaggle/input/paraphrasedistilrobertabasev1exp2/paraphrase-distilroberta-base-v1-exp2\"\n    sup_model = \"/kaggle/input/lecr-ensemble-data1/xlm-roberta-base\"\n    uns_tokenizer = AutoTokenizer.from_pretrained(uns_model + '/tokenizer')\n    sup_tokenizer = AutoTokenizer.from_pretrained(sup_model + '/tokenizer')\n    pooling = \"mean\"\n    gradient_checkpointing = False\n    add_with_best_prob = False\n    \nclass CFG5:\n    uns_model = \"/kaggle/input/paraphrasemultilingualmpnetbasev4-2/paraphrase-multilingual-mpnet-base-v4\"\n    sup_model = \"/kaggle/input/lecr-ensemble-data1/xlm-roberta-base\"\n    uns_tokenizer = AutoTokenizer.from_pretrained(uns_model + '/tokenizer')\n    sup_tokenizer = AutoTokenizer.from_pretrained(sup_model + '/tokenizer')\n    pooling = \"mean\"\n    gradient_checkpointing = False\n    add_with_best_prob = False\n\"\"\"\n#CFG_list = [CFG1, CFG2, CFG3]\nCFG_list = [CFG1, CFG2, CFG4, CFG5]","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:19:37.324716Z","iopub.execute_input":"2023-02-08T05:19:37.325395Z","iopub.status.idle":"2023-02-08T05:19:51.606122Z","shell.execute_reply.started":"2023-02-08T05:19:37.325360Z","shell.execute_reply":"2023-02-08T05:19:51.605103Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"env: TOKENIZERS_PARALLELISM=false\n","output_type":"stream"}]},{"cell_type":"markdown","source":"* title only の場合に良い結果となると記載されていたものの、果たして,,,\n* データの読み込みを行う。ただし、不要な列を切り落とす。\n* 現状のneighborは1000であるが、1200程度まで増加させてみる。","metadata":{}},{"cell_type":"code","source":"def read_data(cfg):\n    topics = pd.read_csv('/kaggle/input/learning-equality-curriculum-recommendations/topics.csv')\n    content = pd.read_csv('/kaggle/input/learning-equality-curriculum-recommendations/content.csv')\n    sample_submission = pd.read_csv('/kaggle/input/learning-equality-curriculum-recommendations/sample_submission.csv')\n    # Merge topics with sample submission to only infer test topics\n    topics = topics.merge(sample_submission, how = 'inner', left_on = 'id', right_on = 'topic_id')\n    # Fillna titles\n    topics['title'].fillna(\"\", inplace = True)\n    content['title'].fillna(\"\", inplace = True)\n    # Sort by title length to make inference faster\n    topics['length'] = topics['title'].apply(lambda x: len(x))\n    content['length'] = content['title'].apply(lambda x: len(x))\n    topics.sort_values('length', inplace = True)\n    content.sort_values('length', inplace = True)\n    # Drop cols\n    topics.drop(['description', 'channel', 'category', 'level', 'parent', 'has_content', 'length', 'topic_id', 'content_ids'], axis = 1, inplace = True)\n    content.drop(['description', 'kind', 'text', 'copyright_holder', 'license', 'length'], axis = 1, inplace = True)\n    # Reset index\n    topics.reset_index(drop = True, inplace = True)\n    content.reset_index(drop = True, inplace = True)\n    print(' ')\n    print('-' * 50)\n    print(f\"topics.shape: {topics.shape}\")\n    print(f\"content.shape: {content.shape}\")\n    return topics, content\n\ndef prepare_uns_input(text, cfg):\n    inputs = cfg.uns_tokenizer.encode_plus(\n        text, \n        return_tensors = None, \n        add_special_tokens = True, \n    )\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype = torch.long)\n    return inputs\n\n\n# =========================================================================================\n# pooling class\n# =========================================================================================\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\nclass AttentionPooling(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.GELU(),\n            nn.Linear(hidden_size, 1)\n        )\n\n    def forward(self, last_hidden_state, attention_mask):\n        w = self.attention(last_hidden_state).float()\n        w[attention_mask==0]=float('-inf')\n        w = torch.softmax(w,1)\n        attention_embeddings = torch.sum(w * last_hidden_state, dim=1)\n        return attention_embeddings\n\nclass MaxPooling(nn.Module):\n    def __init__(self):\n        super(MaxPooling, self).__init__()       \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        embeddings = last_hidden_state.clone()\n        embeddings[input_mask_expanded == 0] = -1e4\n        max_embeddings, _ = torch.max(embeddings, dim = 1)\n        return max_embeddings\n    \n    \nclass MinPooling(nn.Module):\n    def __init__(self):\n        super(MinPooling, self).__init__()     \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        embeddings = last_hidden_state.clone()\n        embeddings[input_mask_expanded == 0] = 1e-4\n        min_embeddings, _ = torch.min(embeddings, dim = 1)\n        return min_embeddings\n    \n\nclass WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, features):\n        ft_all_layers = features['all_layer_embeddings']\n\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        features.update({'token_embeddings': weighted_average})\n        return features\n    \nclass ConcatPooling(nn.Module):\n    def __init__(self, backbone_config, pooling_config):\n        super(ConcatPooling, self, ).__init__()\n\n        self.n_layers = pooling_config.n_layers\n        self.output_dim = backbone_config.hidden_size*pooling_config.n_layers\n\n    def forward(self, inputs, backbone_outputs):\n        all_hidden_states = get_all_hidden_states(backbone_outputs)\n\n        concatenate_pooling = torch.cat([all_hidden_states[-(i + 1)] for i in range(self.n_layers)], -1)\n        concatenate_pooling = concatenate_pooling[:, 0]\n        return concatenate_pooling\n\n# =========================================================================================\n# Unsupervised dataset\n# =========================================================================================\nclass uns_dataset(Dataset):\n    def __init__(self, df, cfg):\n        self.cfg = cfg\n        self.texts = df['title'].values\n    def __len__(self):\n        return len(self.texts)\n    def __getitem__(self, item):\n        inputs = prepare_uns_input(self.texts[item], self.cfg)\n        return inputs\n\n# =========================================================================================\n# Prepare input, tokenize\n# =========================================================================================\ndef prepare_sup_input(text, cfg):\n    inputs = cfg.sup_tokenizer.encode_plus(\n        text, \n        return_tensors = None, \n        add_special_tokens = True, \n    )\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype = torch.long)\n    return inputs\n\n# =========================================================================================\n# Supervised dataset\n# =========================================================================================\nclass sup_dataset(Dataset):\n    def __init__(self, df, cfg):\n        self.cfg = cfg\n        self.texts = df['text'].values\n    def __len__(self):\n        return len(self.texts)\n    def __getitem__(self, item):\n        inputs = prepare_sup_input(self.texts[item], self.cfg)\n        return inputs\n\n# =========================================================================================\n# Unsupervised model\n# =========================================================================================\nclass uns_model(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.config = AutoConfig.from_pretrained(cfg.uns_model + '/config')\n        self.model = AutoModel.from_pretrained(cfg.uns_model + '/model', config = self.config)\n        self.pool = MeanPooling()\n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_state = outputs.last_hidden_state\n        feature = self.pool(last_hidden_state, inputs['attention_mask'])\n        return feature\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        return feature\n\n# =========================================================================================\n# Get embeddings\n# =========================================================================================\ndef get_embeddings(loader, model, device):\n    model.eval()\n    preds = []\n    for step, inputs in enumerate(tqdm(loader)):\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.to('cpu').numpy())\n    preds = np.concatenate(preds)\n    return preds\n\n\n# =========================================================================================\n# Get the amount of positive classes based on the total\n# =========================================================================================\ndef get_pos_socre(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    int_true = np.array([len(x[0] & x[1]) / len(x[0]) for x in zip(y_true, y_pred)])\n    return round(np.mean(int_true), 5)\n\n# =========================================================================================\n# Build our inference set\n# =========================================================================================\ndef build_inference_set(topics, content, cfg):\n    # Create lists for training\n    topics_ids = []\n    content_ids = []\n    topics_languages = []\n    content_languages = []\n    title1 = []\n    title2 = []\n    # Iterate over each topic\n    for k in tqdm(range(len(topics))):\n        row = topics.iloc[k]\n        topics_id = row['id']\n        topics_language = row['language']\n        topics_title = row['title']\n        predictions = row['predictions'].split(' ')\n        for pred in predictions:\n            content_title = content.loc[pred, 'title']\n            content_language = content.loc[pred, 'language']\n            topics_ids.append(topics_id)\n            content_ids.append(pred)\n            title1.append(topics_title)\n            title2.append(content_title)\n            topics_languages.append(topics_language)\n            content_languages.append(content_language)\n    # Build training dataset\n    test = pd.DataFrame(\n        {'topics_ids': topics_ids, \n         'content_ids': content_ids, \n         'title1': title1, \n         'title2': title2,\n         'topic_language': topics_languages, \n         'content_language': content_languages, \n        }\n    )\n    # Release memory\n    del topics_ids, content_ids, title1, title2, topics_languages, content_languages\n    gc.collect()\n    \n    return test\n    \n# =========================================================================================\n# Get neighbors\n# =========================================================================================\n\ndef get_neighbors(tmp_topics, tmp_content, cfg):\n    # Create topics dataset\n    topics_dataset = uns_dataset(tmp_topics, cfg)\n    # Create content dataset\n    content_dataset = uns_dataset(tmp_content, cfg)\n    # Create topics and content dataloaders\n    topics_loader = DataLoader(\n        topics_dataset, \n        batch_size = 32, \n        shuffle = False, \n        collate_fn = DataCollatorWithPadding(tokenizer = cfg.uns_tokenizer, padding = 'longest'),\n        num_workers = 4, \n        pin_memory = True, \n        drop_last = False\n    )\n    content_loader = DataLoader(\n        content_dataset, \n        batch_size = 32, \n        shuffle = False, \n        collate_fn = DataCollatorWithPadding(tokenizer = cfg.uns_tokenizer, padding = 'longest'),\n        num_workers = 4, \n        pin_memory = True, \n        drop_last = False\n        )\n    # Create unsupervised model to extract embeddings\n    model = uns_model(cfg)\n    model.to(device)\n    # Predict topics\n    topics_preds = get_embeddings(topics_loader, model, device)\n    content_preds = get_embeddings(content_loader, model, device)\n    # Transfer predictions to gpu\n    topics_preds_gpu = cp.array(topics_preds)\n    content_preds_gpu = cp.array(content_preds)\n    # Release memory\n    del topics_dataset, content_dataset, topics_loader, content_loader, topics_preds, content_preds\n    gc.collect()\n    torch.cuda.empty_cache()\n    # KNN model\n    print(' ')\n    print('Training KNN model...')\n    neighbors_model = NearestNeighbors(n_neighbors = 1050, metric = 'cosine')\n    neighbors_model.fit(content_preds_gpu)\n    indices = neighbors_model.kneighbors(topics_preds_gpu, return_distance = False)\n    predictions = []\n    for k in range(len(indices)):\n        pred = indices[k]\n        p = ' '.join([tmp_content.loc[ind, 'id'] for ind in pred.get()])\n        predictions.append(p)\n    tmp_topics['predictions'] = predictions\n    # Release memory\n    del topics_preds_gpu, content_preds_gpu, neighbors_model, predictions, indices, model\n    gc.collect()\n    torch.cuda.empty_cache()\n    return tmp_topics, tmp_content\n\n# =========================================================================================\n# Process test\n# =========================================================================================\ndef preprocess_test(tmp_test):\n    tmp_test['title1'].fillna(\"Title does not exist\", inplace = True)\n    tmp_test['title2'].fillna(\"Title does not exist\", inplace = True)\n    # Create feature column\n    tmp_test['text'] = tmp_test['title1'] + '[SEP]' + tmp_test['title2']\n    # Drop titles\n    tmp_test.drop(['title1', 'title2'], axis = 1, inplace = True)\n    # Sort so inference is faster\n    tmp_test['length'] = tmp_test['text'].apply(lambda x: len(x))\n    tmp_test.sort_values('length', inplace = True)\n    tmp_test.drop(['length'], axis = 1, inplace = True)\n    tmp_test.reset_index(drop = True, inplace = True)\n    gc.collect()\n    torch.cuda.empty_cache()\n    return tmp_test\n\n# =========================================================================================\n# Model\n# =========================================================================================\nclass custom_model(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.config = AutoConfig.from_pretrained(cfg.sup_model + '/config', output_hidden_states = True)\n        self.config.hidden_dropout = 0.0\n        self.config.hidden_dropout_prob = 0.0\n        self.config.attention_dropout = 0.0\n        self.config.attention_probs_dropout_prob = 0.0\n        self.model = AutoModel.from_pretrained(cfg.sup_model + '/model', config = self.config)\n        #self.pool = MeanPooling()\n        if self.cfg.gradient_checkpointing:\n            self.model.gradient_checkpointing_enable()\n        if CFG.pooling == 'mean' or CFG.pooling == \"ConcatPool\":\n            self.pool = MeanPooling()\n        elif CFG.pooling == 'max':\n            self.pool = MaxPooling()\n        elif CFG.pooling == 'min':\n            self.pool = MinPooling()\n        elif CFG.pooling == 'attention':\n            self.pool = AttentionPooling(self.config.hidden_size)\n        elif CFG.pooling == \"WLP\":\n            self.pool = WeightedLayerPooling(self.config.num_hidden_layers, layer_start=6)\n        \n        if CFG.pooling == \"ConcatPool\":\n            self.fc = nn.Linear(self.config.hidden_size*4, 1)  \n        else:\n            self.fc = nn.Linear(self.config.hidden_size, 1)\n        #self.fc = nn.Linear(self.config.hidden_size, 1)\n        self._init_weights(self.fc)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        \n        if CFG.pooling == \"WLP\":\n            last_hidden_state = self.model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n            tmp = {\n                'all_layer_embeddings': last_hidden_state.hidden_states\n            }\n            feature = self.pool(tmp)['token_embeddings'][:, 0]\n            \n        elif CFG.pooling == \"ConcatPool\":\n            last_hidden_state = torch.stack(self.model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask']).hidden_states)\n\n            p1 = self.pool(last_hidden_state[-1], inputs['attention_mask'])\n            p2 = self.pool(last_hidden_state[-2], inputs['attention_mask'])\n            p3 = self.pool(last_hidden_state[-3], inputs['attention_mask'])\n            p4 = self.pool(last_hidden_state[-4], inputs['attention_mask'])\n\n            feature = torch.cat(\n                (p1, p2, p3, p4),-1\n            )\n               \n        else:\n            last_hidden_state = outputs.last_hidden_state\n            feature = self.pool(last_hidden_state, inputs['attention_mask'])\n        \n        #last_hidden_state = outputs.last_hidden_state\n        #feature = self.pool(last_hidden_state, inputs['attention_mask'])\n        return feature\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(feature)\n        return output\n# =========================================================================================\n# Inference function loop\n# =========================================================================================\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total = len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.sigmoid().squeeze().to('cpu').numpy().reshape(-1))\n    predictions = np.concatenate(preds)\n    return predictions\n\n# =========================================================================================\n# Inference\n# XLM-roberta は別のノートブックで学習した重みを使用している。\n# そして、推論用モデルとしてXLM-robertaは共通である。\n# =========================================================================================\ndef inference(test, cfg, _idx):\n    # Create dataset and loader\n    test_dataset = sup_dataset(test, cfg)\n    test_loader = DataLoader(\n        test_dataset, \n        batch_size = 32, \n        shuffle = False, \n        collate_fn = DataCollatorWithPadding(tokenizer = cfg.sup_tokenizer, padding = 'longest'),\n        num_workers = 2, \n        pin_memory = True, \n        drop_last = False\n    )\n    # Get model\n    model = custom_model(cfg)\n    \n    # Load weights\n    state = torch.load(\"/kaggle/input/lecr-ensemble-data1/xlm-roberta-base_fold0_42.pth\", map_location = torch.device('cpu'))\n    #if cfg == CFG4 or cfg == CFG5\n    #state = torch.load(\"/kaggle/input/lecr-ensemble-data1/xlm-roberta-large_fold0_42.pth\", map_location = torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    \n    # Release memory\n    torch.cuda.empty_cache()\n    del test_dataset, test_loader, model, state\n    gc.collect()\n    \n    # Use threshold\n    test['probs'] = prediction\n    test['predictions'] = test['probs'].apply(lambda x: int(x > 0.0006))\n    # if cfg == CFG4 or cfg == CFG5\n    #test['predictions'] = test['probs'].apply(lambda x: int(x > 0.0006))\n    test = test.merge(test.groupby(\"topics_ids\", as_index=False)[\"probs\"].max(), on=\"topics_ids\", suffixes=[\"\", \"_max\"])\n    display(test.head())\n    \n    test1 = test[(test['predictions'] == 1) & (test['topic_language'] == test['content_language'])]\n    test1 = test1.groupby(['topics_ids'])['content_ids'].unique().reset_index()\n    test1['content_ids'] = test1['content_ids'].apply(lambda x: ' '.join(x))\n    test1.columns = ['topic_id', 'content_ids']\n    display(test1.head())\n    \n    test0 = pd.Series(test['topics_ids'].unique())\n    test0 = test0[~test0.isin(test1['topic_id'])]\n    test0 = pd.DataFrame({'topic_id': test0.values, 'content_ids': \"\"})\n    if cfg.add_with_best_prob:\n        test0 = test0[[\"topic_id\"]].merge(test[test['probs'] == test['probs_max']][[\"topics_ids\", \"content_ids\"]],\n                                          left_on=\"topic_id\", right_on=\"topics_ids\")[['topic_id', \"content_ids\"]]\n    display(test0.head())\n    test_r = pd.concat([test1, test0], axis = 0, ignore_index = True)\n    test_r.to_csv(f'submission_{_idx+1}.csv', index = False)\n    \n    return test_r","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:19:51.608685Z","iopub.execute_input":"2023-02-08T05:19:51.609406Z","iopub.status.idle":"2023-02-08T05:19:51.682220Z","shell.execute_reply.started":"2023-02-08T05:19:51.609368Z","shell.execute_reply":"2023-02-08T05:19:51.681172Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"* Inference()メソッド内でsubmission_x.csvが保存されている\n* explodeメソッドはデータフレーム内の配列を展開する","metadata":{}},{"cell_type":"code","source":"for _idx, CFG in enumerate(CFG_list):\n    # Read data\n    tmp_topics, tmp_content = read_data(CFG)\n    # Run nearest neighbors\n    tmp_topics, tmp_content = get_neighbors(tmp_topics, tmp_content, CFG)\n    gc.collect()\n    torch.cuda.empty_cache()\n    # Set id as index for content\n    tmp_content.set_index('id', inplace = True)\n    # Build training set\n    tmp_test = build_inference_set(tmp_topics, tmp_content, CFG)\n    # Process test set\n    tmp_test = preprocess_test(tmp_test)\n    # Inference\n    inference(tmp_test, CFG, _idx)\n    del tmp_topics, tmp_content, tmp_test\n    gc.collect()\n    torch.cuda.empty_cache()\n    \ndf_test = pd.concat([pd.read_csv(f'submission_{_idx + 1}.csv') for _idx in range(len(CFG_list))])\ndf_test.fillna(\"\", inplace = True)\ndf_test['content_ids'] = df_test['content_ids'].apply(lambda c: c.split(' '))\ndf_test = df_test.explode('content_ids').groupby(['topic_id'])['content_ids'].unique().reset_index()\ndf_test['content_ids'] = df_test['content_ids'].apply(lambda c: ' '.join(c))\n\ndf_test.to_csv('submission.csv', index = False)\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:19:51.684175Z","iopub.execute_input":"2023-02-08T05:19:51.684878Z","iopub.status.idle":"2023-02-08T05:34:14.694375Z","shell.execute_reply.started":"2023-02-08T05:19:51.684843Z","shell.execute_reply":"2023-02-08T05:34:14.693159Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":" \n--------------------------------------------------\ntopics.shape: (5, 3)\ncontent.shape: (154047, 3)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0fb93233e9848eabcff12b83eb6c8d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4814 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77d8bc556f314eb390bfda61e9953337"}},"metadata":{}},{"name":"stderr","text":"Exception in thread QueueFeederThread:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n    close()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n    self._close()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n    _close(self._handle)\nOSError: [Errno 9] Bad file descriptor\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n    queue_sem.release()\nValueError: semaphore or lock released too many times\n\n","output_type":"stream"},{"name":"stdout","text":" \nTraining KNN model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce517805cbe64b4ba1160100976e4bfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/165 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b59b075b0dd14aa78cf1482ef9a2fe0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"       topics_ids     content_ids topic_language content_language  \\\n0  t_00069b63a70a  c_66111e868395             en               en   \n1  t_00069b63a70a  c_c152775f6f7b             en               en   \n2  t_00069b63a70a  c_f9389c635f87             en               en   \n3  t_00069b63a70a  c_bd5e71a65b93             en               en   \n4  t_00069b63a70a  c_b68d68a3868b             en               en   \n\n                  text         probs  predictions  probs_max  \n0  Transcripts[SEP]DNA  4.851724e-08            0   0.000185  \n1  Transcripts[SEP]DNA  4.851724e-08            0   0.000185  \n2  Transcripts[SEP]RNA  4.613230e-08            0   0.000185  \n3  Transcripts[SEP]DNA  4.851724e-08            0   0.000185  \n4  Transcripts[SEP]DNA  4.851724e-08            0   0.000185  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topics_ids</th>\n      <th>content_ids</th>\n      <th>topic_language</th>\n      <th>content_language</th>\n      <th>text</th>\n      <th>probs</th>\n      <th>predictions</th>\n      <th>probs_max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>t_00069b63a70a</td>\n      <td>c_66111e868395</td>\n      <td>en</td>\n      <td>en</td>\n      <td>Transcripts[SEP]DNA</td>\n      <td>4.851724e-08</td>\n      <td>0</td>\n      <td>0.000185</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>t_00069b63a70a</td>\n      <td>c_c152775f6f7b</td>\n      <td>en</td>\n      <td>en</td>\n      <td>Transcripts[SEP]DNA</td>\n      <td>4.851724e-08</td>\n      <td>0</td>\n      <td>0.000185</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>t_00069b63a70a</td>\n      <td>c_f9389c635f87</td>\n      <td>en</td>\n      <td>en</td>\n      <td>Transcripts[SEP]RNA</td>\n      <td>4.613230e-08</td>\n      <td>0</td>\n      <td>0.000185</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>t_00069b63a70a</td>\n      <td>c_bd5e71a65b93</td>\n      <td>en</td>\n      <td>en</td>\n      <td>Transcripts[SEP]DNA</td>\n      <td>4.851724e-08</td>\n      <td>0</td>\n      <td>0.000185</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>t_00069b63a70a</td>\n      <td>c_b68d68a3868b</td>\n      <td>en</td>\n      <td>en</td>\n      <td>Transcripts[SEP]DNA</td>\n      <td>4.851724e-08</td>\n      <td>0</td>\n      <td>0.000185</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"         topic_id                                        content_ids\n0  t_00004da3a1b2                      c_1108dd0c7a5d c_5bc0e1e2cba0\n1  t_00068291e9a4  c_ebb7fdf10a7e c_14bf71640ecd c_ac1672cdcd2c c...\n2  t_0006d41a73a8  c_b972646631cb c_0c6473c3480d c_d7a0d7eaf799 c...\n3  t_4054df11a74e                                     c_3695c5dc1df6","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic_id</th>\n      <th>content_ids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>t_00004da3a1b2</td>\n      <td>c_1108dd0c7a5d c_5bc0e1e2cba0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>t_00068291e9a4</td>\n      <td>c_ebb7fdf10a7e c_14bf71640ecd c_ac1672cdcd2c c...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>t_0006d41a73a8</td>\n      <td>c_b972646631cb c_0c6473c3480d c_d7a0d7eaf799 c...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>t_4054df11a74e</td>\n      <td>c_3695c5dc1df6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"         topic_id     content_ids\n0  t_00069b63a70a  c_749b9bfd3a69","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic_id</th>\n      <th>content_ids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>t_00069b63a70a</td>\n      <td>c_749b9bfd3a69</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":" \n--------------------------------------------------\ntopics.shape: (5, 3)\ncontent.shape: (154047, 3)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cbfd44e491c4dd2b78166b98e931ed6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4814 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6066f3c82cf249e8a8c2e3f0af0f78a9"}},"metadata":{}},{"name":"stderr","text":"Exception in thread QueueFeederThread:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n    close()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n    self._close()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n    _close(self._handle)\nOSError: [Errno 9] Bad file descriptor\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n    queue_sem.release()\nValueError: semaphore or lock released too many times\n\nException in thread QueueFeederThread:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n    close()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n    self._close()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n    _close(self._handle)\nOSError: [Errno 9] Bad file descriptor\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n    queue_sem.release()\nValueError: semaphore or lock released too many times\n\nException in thread QueueFeederThread:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n    close()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n    self._close()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n    _close(self._handle)\nOSError: [Errno 9] Bad file descriptor\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n    queue_sem.release()\nValueError: semaphore or lock released too many times\n\n","output_type":"stream"},{"name":"stdout","text":" \nTraining KNN model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31ce6f3509c941b3a12900a1db125e39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/165 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7741981f9fcd41329e52760e5de908f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"       topics_ids     content_ids topic_language content_language  \\\n0  t_00069b63a70a  c_c152775f6f7b             en               en   \n1  t_00069b63a70a  c_bd5e71a65b93             en               en   \n2  t_00069b63a70a  c_a7799481219a             en               pt   \n3  t_00069b63a70a  c_f9389c635f87             en               en   \n4  t_00069b63a70a  c_b68d68a3868b             en               en   \n\n                  text         probs  predictions  probs_max  \n0  Transcripts[SEP]DNA  4.851724e-08            0    0.00019  \n1  Transcripts[SEP]DNA  4.851724e-08            0    0.00019  \n2  Transcripts[SEP]DNA  4.851724e-08            0    0.00019  \n3  Transcripts[SEP]RNA  4.613230e-08            0    0.00019  \n4  Transcripts[SEP]DNA  4.851724e-08            0    0.00019  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topics_ids</th>\n      <th>content_ids</th>\n      <th>topic_language</th>\n      <th>content_language</th>\n      <th>text</th>\n      <th>probs</th>\n      <th>predictions</th>\n      <th>probs_max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>t_00069b63a70a</td>\n      <td>c_c152775f6f7b</td>\n      <td>en</td>\n      <td>en</td>\n      <td>Transcripts[SEP]DNA</td>\n      <td>4.851724e-08</td>\n      <td>0</td>\n      <td>0.00019</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>t_00069b63a70a</td>\n      <td>c_bd5e71a65b93</td>\n      <td>en</td>\n      <td>en</td>\n      <td>Transcripts[SEP]DNA</td>\n      <td>4.851724e-08</td>\n      <td>0</td>\n      <td>0.00019</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>t_00069b63a70a</td>\n      <td>c_a7799481219a</td>\n      <td>en</td>\n      <td>pt</td>\n      <td>Transcripts[SEP]DNA</td>\n      <td>4.851724e-08</td>\n      <td>0</td>\n      <td>0.00019</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>t_00069b63a70a</td>\n      <td>c_f9389c635f87</td>\n      <td>en</td>\n      <td>en</td>\n      <td>Transcripts[SEP]RNA</td>\n      <td>4.613230e-08</td>\n      <td>0</td>\n      <td>0.00019</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>t_00069b63a70a</td>\n      <td>c_b68d68a3868b</td>\n      <td>en</td>\n      <td>en</td>\n      <td>Transcripts[SEP]DNA</td>\n      <td>4.851724e-08</td>\n      <td>0</td>\n      <td>0.00019</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"         topic_id                                        content_ids\n0  t_00004da3a1b2                      c_1108dd0c7a5d c_5bc0e1e2cba0\n1  t_00068291e9a4  c_639ea2ef9c95 c_ebb7fdf10a7e c_14bf71640ecd c...\n2  t_0006d41a73a8  c_0c6473c3480d c_b972646631cb c_d7a0d7eaf799 c...\n3  t_4054df11a74e                                     c_3695c5dc1df6","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic_id</th>\n      <th>content_ids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>t_00004da3a1b2</td>\n      <td>c_1108dd0c7a5d c_5bc0e1e2cba0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>t_00068291e9a4</td>\n      <td>c_639ea2ef9c95 c_ebb7fdf10a7e c_14bf71640ecd c...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>t_0006d41a73a8</td>\n      <td>c_0c6473c3480d c_b972646631cb c_d7a0d7eaf799 c...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>t_4054df11a74e</td>\n      <td>c_3695c5dc1df6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"         topic_id     content_ids\n0  t_00069b63a70a  c_186fc761585b","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic_id</th>\n      <th>content_ids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>t_00069b63a70a</td>\n      <td>c_186fc761585b</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":" \n--------------------------------------------------\ntopics.shape: (5, 3)\ncontent.shape: (154047, 3)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7251305b59d0452c8e064bfd152e952d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4814 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c35cf871ed634f86872d6c4cb12afb34"}},"metadata":{}},{"name":"stderr","text":"Exception in thread QueueFeederThread:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n    close()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n    self._close()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n    _close(self._handle)\nOSError: [Errno 9] Bad file descriptor\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n    queue_sem.release()\nValueError: semaphore or lock released too many times\n\n","output_type":"stream"},{"name":"stdout","text":" \nTraining KNN model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83b0469108744f84b085c552d7fea7a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/165 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fef00aa4f17444db9818bbf966f150f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"       topics_ids     content_ids topic_language content_language  \\\n0  t_00069b63a70a  c_83398bf2a6b9             en               ar   \n1  t_00069b63a70a  c_2aeda03b182e             en               ar   \n2  t_00069b63a70a  c_293622bd38b5             en               ar   \n3  t_00069b63a70a  c_835550e73df2             en               ar   \n4  t_00069b63a70a  c_f0500a6b4e87             en               ar   \n\n                            text         probs  predictions  probs_max  \n0        Transcripts[SEP]التناسب  2.698774e-08            0    0.00019  \n1      Transcripts[SEP]\"سوبرمان\"  2.902284e-08            0    0.00019  \n2    Transcripts[SEP]رغمًا عن...  1.029451e-08            0    0.00019  \n3  Transcripts[SEP]قانون فاراداي  2.733270e-08            0    0.00019  \n4  Transcripts[SEP]\"سوبِّر ماما\"  3.247516e-08            0    0.00019  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topics_ids</th>\n      <th>content_ids</th>\n      <th>topic_language</th>\n      <th>content_language</th>\n      <th>text</th>\n      <th>probs</th>\n      <th>predictions</th>\n      <th>probs_max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>t_00069b63a70a</td>\n      <td>c_83398bf2a6b9</td>\n      <td>en</td>\n      <td>ar</td>\n      <td>Transcripts[SEP]التناسب</td>\n      <td>2.698774e-08</td>\n      <td>0</td>\n      <td>0.00019</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>t_00069b63a70a</td>\n      <td>c_2aeda03b182e</td>\n      <td>en</td>\n      <td>ar</td>\n      <td>Transcripts[SEP]\"سوبرمان\"</td>\n      <td>2.902284e-08</td>\n      <td>0</td>\n      <td>0.00019</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>t_00069b63a70a</td>\n      <td>c_293622bd38b5</td>\n      <td>en</td>\n      <td>ar</td>\n      <td>Transcripts[SEP]رغمًا عن...</td>\n      <td>1.029451e-08</td>\n      <td>0</td>\n      <td>0.00019</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>t_00069b63a70a</td>\n      <td>c_835550e73df2</td>\n      <td>en</td>\n      <td>ar</td>\n      <td>Transcripts[SEP]قانون فاراداي</td>\n      <td>2.733270e-08</td>\n      <td>0</td>\n      <td>0.00019</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>t_00069b63a70a</td>\n      <td>c_f0500a6b4e87</td>\n      <td>en</td>\n      <td>ar</td>\n      <td>Transcripts[SEP]\"سوبِّر ماما\"</td>\n      <td>3.247516e-08</td>\n      <td>0</td>\n      <td>0.00019</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"         topic_id                                        content_ids\n0  t_00004da3a1b2                      c_1108dd0c7a5d c_5bc0e1e2cba0\n1  t_00068291e9a4  c_639ea2ef9c95 c_ebb7fdf10a7e c_14bf71640ecd c...\n2  t_0006d41a73a8  c_0c6473c3480d c_b972646631cb c_d7a0d7eaf799 c...\n3  t_4054df11a74e                                     c_3695c5dc1df6","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic_id</th>\n      <th>content_ids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>t_00004da3a1b2</td>\n      <td>c_1108dd0c7a5d c_5bc0e1e2cba0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>t_00068291e9a4</td>\n      <td>c_639ea2ef9c95 c_ebb7fdf10a7e c_14bf71640ecd c...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>t_0006d41a73a8</td>\n      <td>c_0c6473c3480d c_b972646631cb c_d7a0d7eaf799 c...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>t_4054df11a74e</td>\n      <td>c_3695c5dc1df6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"         topic_id     content_ids\n0  t_00069b63a70a  c_186fc761585b","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic_id</th>\n      <th>content_ids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>t_00069b63a70a</td>\n      <td>c_186fc761585b</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"         topic_id                                        content_ids\n0  t_00004da3a1b2                      c_1108dd0c7a5d c_5bc0e1e2cba0\n1  t_00068291e9a4  c_ebb7fdf10a7e c_14bf71640ecd c_ac1672cdcd2c c...\n2  t_00069b63a70a                      c_749b9bfd3a69 c_186fc761585b\n3  t_0006d41a73a8  c_b972646631cb c_0c6473c3480d c_d7a0d7eaf799 c...\n4  t_4054df11a74e                                     c_3695c5dc1df6","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic_id</th>\n      <th>content_ids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>t_00004da3a1b2</td>\n      <td>c_1108dd0c7a5d c_5bc0e1e2cba0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>t_00068291e9a4</td>\n      <td>c_ebb7fdf10a7e c_14bf71640ecd c_ac1672cdcd2c c...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>t_00069b63a70a</td>\n      <td>c_749b9bfd3a69 c_186fc761585b</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>t_0006d41a73a8</td>\n      <td>c_b972646631cb c_0c6473c3480d c_d7a0d7eaf799 c...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>t_4054df11a74e</td>\n      <td>c_3695c5dc1df6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}